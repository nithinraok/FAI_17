{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting into NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from utils import *\n",
    "model_path = '../data/aclImdb/models'\n",
    "%mkdir -p $model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1531904/1641221 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx=imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx,key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word= { v:k for k,v in idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('imdb_full.pkl','https://s3.amazonaws.com/text-datasets/imdb_full.pkl')\n",
    "\n",
    "f=open(path,'rb')\n",
    "(x_train,labels_train),(x_test,labels_test)=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is one of the dumbest films i've ever seen it rips off nearly ever type of thriller and manages to make a mess of them all br br there's not a single good line or character in the whole mess if there was a plot it was an afterthought and as far as acting goes there's nothing good to say so ill say nothing i honestly cant understand how this type of nonsense gets produced and actually released does somebody somewhere not at some stage think 'oh my god this really is a load of shite' and call it a day its crap like this that has people downloading illegally the trailer looks like a completely different film at least if you have download it you haven't wasted your time or money don't waste your time this is painful\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing vocab size\n",
    "vocab_size= 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is one of the bergman films i've ever seen it bergman off nearly ever type of thriller and manages to make a mess of them all br br there's not a single good line or character in the whole mess if there was a plot it was an bergman and as far as acting goes there's nothing good to say so ill say nothing i honestly cant understand how this type of nonsense gets produced and actually released does somebody somewhere not at some stage think bergman my god this really is a load of bergman and call it a day its crap like this that has people bergman bergman the trailer looks like a completely different film at least if you have bergman it you haven't wasted your time or money don't waste your time this is painful\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in trn[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[len(trn[i]) for i in range(len(trn))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max(lens),min(lens),np.mean(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making all sentences of equal length\n",
    "seq_len = 500\n",
    "\n",
    "trn=sequence.pad_sequences(trn,maxlen=seq_len,value=0)\n",
    "test=sequence.pad_sequences(test,maxlen=seq_len,value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels_trn=np.asarray(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model=Sequential([Embedding(vocab_size,32,input_length=seq_len),\n",
    "                 Flatten(),\n",
    "                 Dense(100,activation='relu'),\n",
    "                 Dropout(0.7),\n",
    "                 Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 14s - loss: 0.4710 - acc: 0.7408 - val_loss: 0.2899 - val_acc: 0.8770\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 14s - loss: 0.2027 - acc: 0.9230 - val_loss: 0.3013 - val_acc: 0.8735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c2d2240>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn,labels_trn,validation_data=(test,labels_test),epochs=2,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1D-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential([\n",
    "    Embedding(vocab_size,32,input_length=seq_len),\n",
    "    keras.layers.SpatialDropout1D(0.2),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64,5,padding='same',activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100,activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 51s - loss: 0.5225 - acc: 0.6998 - val_loss: 0.2914 - val_acc: 0.8807\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 51s - loss: 0.2800 - acc: 0.8963 - val_loss: 0.2572 - val_acc: 0.8939\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 51s - loss: 0.2286 - acc: 0.9176 - val_loss: 0.2619 - val_acc: 0.8924\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 51s - loss: 0.1958 - acc: 0.9293 - val_loss: 0.2761 - val_acc: 0.8897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1296ad320>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit(trn,labels_trn,validation_data=(test,labels_test),epochs=4,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('../data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "           pickle.load(open(loc+'_words.pkl','rb'),encoding='utf-8'),\n",
    "           pickle.load(open(loc+'_idx.pkl','rb'),encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01, -4.4457e-02, -4.9688e-01,\n",
       "       -1.7862e-01, -6.6023e-04, -6.5660e-01,  2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01,\n",
       "       -9.5095e-03,  1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01, -1.6801e-02,\n",
       "       -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01, -1.8823e+00, -7.6746e-01,  9.9051e-02,\n",
       "       -4.2125e-01, -1.9526e-01,  4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02, -2.9871e-01, -1.5749e-01,\n",
       "       -3.4758e-01, -4.5637e-02, -4.4251e-01,  1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01,\n",
       "       -7.8581e-01], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx2word_glove={v:k for k,v in wordidx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "??normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb=create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3933e-01,  8.3227e-02, -1.3747e-01,  4.0567e-02,  1.1509e-01, -1.4819e-02, -1.6563e-01,\n",
       "       -5.9540e-02, -2.2008e-04, -2.1887e-01,  9.2810e-02, -4.9223e-02, -1.8559e-01,  4.8860e-02,\n",
       "       -3.1698e-03,  3.8860e-03,  3.4013e-02, -4.2640e-02, -2.8143e-01, -4.0603e-02, -5.6003e-03,\n",
       "       -1.1093e-01, -5.1733e-02, -7.7103e-02, -6.3937e-02, -6.2743e-01, -2.5582e-01,  3.3017e-02,\n",
       "       -1.4042e-01, -6.5087e-02,  1.3357e+00, -6.1980e-02, -1.7429e-01, -1.0560e-01,  1.9738e-04,\n",
       "        2.4816e-03,  5.9260e-02, -5.2990e-02,  4.0137e-03, -1.8074e-02, -9.9570e-02, -5.2497e-02,\n",
       "       -1.1586e-01, -1.5212e-02, -1.4750e-01,  6.2617e-02,  9.2830e-04, -6.1370e-02, -3.8380e-02,\n",
       "       -2.6194e-01])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size,50,input_length=seq_len,weights=[emb],trainable=False),\n",
    "    keras.layers.SpatialDropout1D(0.2),\n",
    "    Dropout(0.25),\n",
    "    Conv1D(64,5,padding='same',activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100,activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 64)           16064     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,866,265\n",
      "Trainable params: 1,616,265\n",
      "Non-trainable params: 250,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 61s - loss: 0.5269 - acc: 0.7492 - val_loss: 0.4697 - val_acc: 0.7944\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 60s - loss: 0.5004 - acc: 0.7637 - val_loss: 0.4602 - val_acc: 0.7930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c4885f8>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn,labels_train,validation_data=(test,labels_test),epochs=2,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizertimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 64s - loss: 0.4817 - acc: 0.7773 - val_loss: 0.4281 - val_acc: 0.8196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c453780>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn,labels_train,validation_data=(test,labels_test),epochs=1,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-size *CNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_in = Input((vocab_size,50))\n",
    "convs=[];\n",
    "for fsz in range(3,6):\n",
    "    x=Conv1D(64,fsz,padding='same',activation='relu')(graph_in)\n",
    "    x=MaxPooling1D()(x)\n",
    "    x=Flatten()(x)\n",
    "    convs.append(x)\n",
    "out=concatenate(convs)\n",
    "graph=Model(graph_in,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    Embedding(vocab_size,50,input_length=seq_len,weights=[emb]),\n",
    "    Dropout(0.2),\n",
    "    graph,\n",
    "    Dropout(0.5),\n",
    "    Dense(100,activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 137s - loss: 0.6440 - acc: 0.6076 - val_loss: 0.3579 - val_acc: 0.8546\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 133s - loss: 0.3519 - acc: 0.8627 - val_loss: 0.2667 - val_acc: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a261978>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 134s - loss: 0.2633 - acc: 0.9007 - val_loss: 0.2558 - val_acc: 0.8962\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 134s - loss: 0.2274 - acc: 0.9143 - val_loss: 0.2678 - val_acc: 0.8927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b10d898>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size,32,input_length=seq_len,mask_zero=True,\n",
    "              embeddings_regularizer=l2(1e-6)),\n",
    "    LSTM(100,implementation=2),\n",
    "    Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 583s - loss: 0.5246 - acc: 0.7163 - val_loss: 0.3446 - val_acc: 0.8573\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 580s - loss: 0.3040 - acc: 0.8755 - val_loss: 0.3288 - val_acc: 0.8595\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 580s - loss: 0.2500 - acc: 0.9027 - val_loss: 0.3082 - val_acc: 0.8720\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 570s - loss: 0.2181 - acc: 0.9184 - val_loss: 0.2985 - val_acc: 0.8799\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 3074s - loss: 0.2421 - acc: 0.9050 - val_loss: 0.3108 - val_acc: 0.8756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14654bcf8>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
